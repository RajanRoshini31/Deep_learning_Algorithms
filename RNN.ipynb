{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNJ8WiqB9FpzD+gfuzSf9bq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdiptUUr5ZAp","executionInfo":{"status":"ok","timestamp":1742446977958,"user_tz":-330,"elapsed":74868,"user":{"displayName":"Roshini Rajan","userId":"05105296642670067175"}},"outputId":"190cd7bc-0400-49de-cf4a-26e59fbe6ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - accuracy: 0.5133 - loss: 0.6928 - val_accuracy: 0.6465 - val_loss: 0.6435\n","Epoch 2/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 62ms/step - accuracy: 0.7117 - loss: 0.5956 - val_accuracy: 0.7543 - val_loss: 0.5269\n","Epoch 3/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 52ms/step - accuracy: 0.8051 - loss: 0.4522 - val_accuracy: 0.7856 - val_loss: 0.4535\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7877 - loss: 0.4511\n","\n","Test Accuracy: 0.7856\n"]}],"source":["#Simple RNN Model\n","# Step 1: Load IMDB dataset\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Load the IMDB dataset\n","vocab_size = 10000  # Top 10,000 words\n","max_length = 100    # Max words per review\n","embedding_dim = 16  # Embedding output size\n","\n","# Load dataset\n","(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","\n","# Pad sequences to ensure uniform input size\n","train_data = pad_sequences(train_data, maxlen=max_length, padding='post', truncating='post')\n","test_data = pad_sequences(test_data, maxlen=max_length, padding='post', truncating='post')\n","\n","# Step 2: Build the RNN model\n","model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),  # Word Embeddings\n","    keras.layers.SimpleRNN(64, activation='relu'),  # RNN Layer\n","    keras.layers.Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n","])\n","\n","# Step 3: Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Step 4: Train the model\n","model.fit(train_data, train_labels, epochs=3, batch_size=128, validation_data=(test_data, test_labels))\n","\n","# Step 5: Evaluate the model\n","test_loss, test_acc = model.evaluate(test_data, test_labels)\n","print(f\"\\nTest Accuracy: {test_acc:.4f}\")"]},{"cell_type":"code","source":["#Simple RNN Model\n","# Step 1: Load IMDB dataset\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Load the IMDB dataset\n","vocab_size = 10000  # Top 10,000 words\n","max_length = 100    # Max words per review\n","embedding_dim = 16  # Embedding output size\n","\n","# Load dataset\n","(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","\n","# Pad sequences to ensure uniform input size\n","train_data = pad_sequences(train_data, maxlen=max_length, padding='post', truncating='post')\n","test_data = pad_sequences(test_data, maxlen=max_length, padding='post', truncating='post')\n","\n","# Step 2: Build the RNN model\n","model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),  # Word Embeddings\n","    keras.layers.GRU(64, activation='tanh'),  # RNN Layer\n","    keras.layers.Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n","])\n","\n","# Step 3: Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Step 4: Train the model\n","model.fit(train_data, train_labels, epochs=3, batch_size=128, validation_data=(test_data, test_labels))\n","\n","# Step 5: Evaluate the model\n","test_loss, test_acc = model.evaluate(test_data, test_labels)\n","print(f\"\\nTest Accuracy: {test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFe2O48V7_Ml","executionInfo":{"status":"ok","timestamp":1742448592749,"user_tz":-330,"elapsed":22631,"user":{"displayName":"Roshini Rajan","userId":"05105296642670067175"}},"outputId":"5aa7f6c4-1b07-4da6-ea2a-c3acdbe6fd8b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.5572 - loss: 0.6725 - val_accuracy: 0.8006 - val_loss: 0.4384\n","Epoch 2/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.8425 - loss: 0.3718 - val_accuracy: 0.8117 - val_loss: 0.4405\n","Epoch 3/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.8952 - loss: 0.2776 - val_accuracy: 0.8093 - val_loss: 0.4625\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8092 - loss: 0.4608\n","\n","Test Accuracy: 0.8093\n"]}]},{"cell_type":"code","source":["#Simple RNN Model\n","# Step 1: Load IMDB dataset\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Load the IMDB dataset\n","vocab_size = 10000  # Top 10,000 words\n","max_length = 100    # Max words per review\n","embedding_dim = 16  # Embedding output size\n","\n","# Load dataset\n","(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=vocab_size)\n","\n","# Pad sequences to ensure uniform input size\n","train_data = pad_sequences(train_data, maxlen=max_length, padding='post', truncating='post')\n","test_data = pad_sequences(test_data, maxlen=max_length, padding='post', truncating='post')\n","\n","# Step 2: Build the RNN model\n","model = keras.Sequential([\n","    keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),  # Word Embeddings\n","    keras.layers.LSTM(64, activation='tanh'),  # RNN Layer\n","    keras.layers.Dense(1, activation='sigmoid')  # Output Layer (Binary Classification)\n","])\n","\n","# Step 3: Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Step 4: Train the model\n","model.fit(train_data, train_labels, epochs=3, batch_size=128, validation_data=(test_data, test_labels))\n","\n","# Step 5: Evaluate the model\n","test_loss, test_acc = model.evaluate(test_data, test_labels)\n","print(f\"\\nTest Accuracy: {test_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XD5fj9Vd8XON","executionInfo":{"status":"ok","timestamp":1742448541266,"user_tz":-330,"elapsed":22089,"user":{"displayName":"Roshini Rajan","userId":"05105296642670067175"}},"outputId":"cb962256-3c17-47bc-b78d-4c9b399fc65b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.6169 - loss: 0.6186 - val_accuracy: 0.8081 - val_loss: 0.4174\n","Epoch 2/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8649 - loss: 0.3342 - val_accuracy: 0.8085 - val_loss: 0.4268\n","Epoch 3/3\n","\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9010 - loss: 0.2671 - val_accuracy: 0.7864 - val_loss: 0.5204\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7888 - loss: 0.5163\n","\n","Test Accuracy: 0.7864\n"]}]}]}